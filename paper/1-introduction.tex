\chapter{Introduction}
With the invention and wide adoption of the microprocessor, it first became viable to use computers at home in the late 1970s. The release of the "1977 trinity" \cite, by Apple, Tandy and Commodore marked the early beginnings of the personal computer era. These machines convinced consumers with their ability to run software at an unprecedented performance and complexity level at home. Only thanks to the 8-bit microprocessors powerinig the machines, the personal computer revolutions was possible.

EXPAND ON THIS

Apart from this advancement in the silicon industry, the computer as a whole was only possible due to work, fueled by the second world war, made by many mathematicians. Most significantly \cite{cit.needed} Alan Turing and John Von Neumann independently developed the theoretical frameworks for the computer. The Von Neumann architecture (VNA) is the basis for most modern computers.

\section{Idea and Goal} \label{Goals}


Motivated by reuniting the the technology that made the personal computing revolution a reality and the earliest theoretical framework of the computer, the goal of this project is to: 
\begin{itemize}
  \item Have a functioning computer architecture that:
 \begin{enumerate}
    \item Has an 8-bit bus width; and
    \item Is Turing complete; and
    \item Is based on the Von-Neumann-Architecture (VNA); and
    \item Implements the Von-Neumann-Cycle; and
    \item Implements features wanted by me; and  
    \item Is kept as simple as possible; and
    \item Is, with supporting work, explainable graphically.
  \end{enumerate}
  \item Have a simulation of this computer architecture that: 
  \begin{enumerate}
    \item Is fully tested, ensuring all requirements are met; and
    \item Is kept as simple as possible; and
    \item Can be interacted with by a user; and
    \item Is programmable by a higher level language (assembly); and
    \item Is, with supporting work, explainable graphically.
  \end{enumerate}
  
\end{itemize}

Finally the limitation on the bus width ensures that the complexity of the architecture is limited and can be grasped by a single person.


\section{Theory}
With the development of highly complex chip designs in the late 20th century further and further abstraction of the process was required, ensuring that all stakeholders knew the exact specifications of the design. \cite{1214355} Whilst the earliest chip designs were drawn by hand and transferred onto silicon by photolithography, chip designs nowadays are written in an abstract computer language; a Hardware Description Language. Apart from allowing separation, modularity and reusability of components, this description later also allowed for simulation of the design and thus a reduction of errors in the final physically built design. 

To ensure that the design correctly implements all features, professional chip designs are fully and automatically tested, by simulating real world conditions as well as corner cases. 



Test driven development, unit testing, reproducibility, testing is important ;) no chance to produce working silicon otherwise.

\section{Tools}

The most popular flavor of such an "HDL" is Verilog, as defined in \cite{10458102} and its extensions. Given widespread professional use of Verilog, more specifically the Verilog superset SystemVerilog, seemed to be the best option for this project, a large amount of information and guides on the topic exist. The terms SystemVerilog and Verilog will be used interchangeably. 

As \cite{10458102} only defines the language's syntax, a Verilog tool suite is required. Although previous experience in the usage of Verilog exists, expertise on the intricacies of Verilog simulation is still limited. It was thus decided based on the integration with other tooling, as to which suite is to be used. 

The key feature of the chosen suite is the compilation of the Verilog code to a binary and the generation of an interface to C++. Apart from being able to rely on previous experience in C++, it also allows me to make use of a vast ecosystem of testing, code coverage and DevOps frameworks. I chose the GoogleTest framework for my unit testing. 

Additionally, Verilator counts activations of specific lines of code which is used to produce a report that visualizes if all code paths are tested.

Git and GitLab is used due to previous experience and existing infrastructure.

% \include{1-introduction/verilog-crash-course}

\section{Development Environment}
To reproduce the development environment for this work, the following packages need to be installed with the system package manager. It is suggested to use the GNU+Linux operating system. 

\begin{itemize}
  \item verilator@5.24
  \item a C++ toolchain (e.g. gcc)
  \item cmake
  \item ninja
\end{itemize}
  

Finally, git hooks are set up to ensure that before comitting, all compilation steps complete and the commit message format complies with the conventional commit standard \cite{conventionalcommit}

\section{Development Operations (DevOps)}
Additionally, to increase development velocity and traceability development operations (DevOps) were put into place in the form of tools that are contiously run to provide fast feedback to developers on code by running unit tests and static code analysis. 

The following shall be continuously run whenever a change is committed:

\begin{itemize}
  \item Compilation of all code and of the paper.  
  \item Execution of all test cases and logging of the results to the DevOps platform (GitLab CI)
  \item Code coverage report generation
\end{itemize}

This was realised by leveraging the version control system (git) and the GitLab CI/CD platform. Replicating the local environments, two Debian based docker containers \cite{dockerVerilator} \cite{dockerLatex} were used in the respective pipelines. One for compilation of the architecture and one for the compilation of the paper. The pipelines are set up to report code coverage and test results to the version control system platform. 

As the version control system infrastructure does not provide any infrastructure to execute these pipelines, e.g. servers to run the docker containers, seperate machines were put in place.

\subsection{Requirements}
Given the previously defined goal \ref{goals} three distinct groups of feature requirements to the computer architecture can be differentiated: 

Requirements resulting from the need for Turing completeness \cite{turing1936a}, from here on out referred to as "Turing requirements", are based around the following concepts: increasable memory (thus infinite), ability to modify memory and the ability to execute conditionally. 

Architectural requirements are a set of non-functional requirements given by the intended computer architecture \cite{vonneuman1945a}. They are by design only verifiable and not testable. 

Finally, feature requirements are requirements arbitrarily defined by the author to expand the feature set. These requirements shall be justified, when put in place and tested.

Requirements are inlined and numbered to provide a point of reference when implementing and testing the requirements. 

\newtheorem{turing-requirement}{Turing Req.}[subsection]
\newtheorem{arch-requirement}{Arch. Req.}[subsection]
\newtheorem{feat-requirement}{Feat. Req.}[subsection]

The keywords "must", "must not", "required", "shall", "shall not", "should", "should not", "recommended",  "may", and "optional" in the requirements are to be interpreted as described in RFC 2119 \cite{rfc2119}.

